{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Project 3: Text Generator",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project 3: Text Generator\n",
        "\n",
        "⚠️   **Duplicate this project before you start working on it, using `File > Save a copy in drive`.**\n",
        "\n",
        "The goal of this project is to build a procedural text generator: a program that can generate random text that might actually make sense, based on some original real text. It might generate jokes, wise sayings, movie trailers, restaurant reviews, research papers - that part will be up to you!"
      ],
      "metadata": {
        "id": "9K8ZCt2F9hmJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Markov chain\n",
        "\n",
        "A common approach for text generation these days might be a neural network trained on huge amounts of data, but we'll be using an older technique that requires very little data, and can be programmed entirely from scratch with what you know now in Python. Woo!\n",
        "\n",
        "The technique is called a **Markov chain** and is used in many industries to model sequences of events based on probabilities. Here, the sequence of events is the sequence of words in an English sentence, and the probabilities are the likelihood that one word is followed by another word.\n",
        "\n",
        "For example, we could create a model based solely on these sentences:\n",
        "\n",
        "* I am mad.\n",
        "* I am happy.\n",
        "* I love you.\n",
        "* I want cookies.\n",
        "* I love cookies.\n",
        "\n",
        "In this limited language (perhaps spoken by a toddler), the start word of every sentence is always \"I\". The next word has a 40% probability of being \"am\", a 40% probability of being \"love\", and a 20% probability of being \"want\". \n",
        "\n",
        "![Markov chain for I probabilities](https://corise.com/static/course/introduction-to-python/assets/cky534a6f00l01ga176er12br/markov_i.png)\n",
        "\n",
        "\n",
        "We can also compute the probabilities for the other words in the language. The word \"am\" has a 50% probability of being followed by \"mad\" and 50% probability of being followed by \"happy\". The word \"love\" has the same probabilities for \"you\" and \"cookies\". \n",
        "\n",
        "![Markov chain for am and love probabilities](https://corise.com/static/course/introduction-to-python/assets/cky537lac00l61ga18if91w9c/markov_amlove.png)\n",
        "\n",
        "The word \"want\" is only ever followed by \"cookies\" (toddlers know where it's at).\n",
        "\n",
        "![Markov chain for want probability](https://corise.com/static/course/introduction-to-python/assets/cky53j7wf00lb1ga15xpl85c9/markov_want.png)\n",
        "\n",
        "\n",
        "The remaining words all have a 100% probability of being followed by a period, meaning that they're always the last word in a sentence.\n",
        "\n",
        "![Markov chain for probabilities of you, cookies, mad, happy](https://corise.com/static/course/introduction-to-python/assets/cky532fpk00kr1ga16t6s9yjx/markov_madhappycookiesyou.png)\n",
        "\n",
        "\n",
        "We've now modeled this tiny language according to the probabilities of one word following another word, and we can use this model to generate sentences. We just start from a start word, which is always \"I\" in this language, and then pick the next word based on the probabilities and random dice rolls. Eventually, we'd generate all the original sentences in the language.\n",
        "\n",
        "But that's not very exciting, we want to generate new sentences! Well, if we added more and longer sentences to the input data, we'd likely eventually end up generating never-before-seen sentences. For example, if \"you want cookies\" was an input sentence, then the chain could generate \"I love you want cookies\", because it would observe that \"you\" could be followed by either a period or \"want\". \n",
        "\n",
        "![New sentence generated by chain](https://corise.com/static/course/introduction-to-python/assets/cky53rh0n00li1ga1crhkcsii/markov_newsentence.png)\n",
        "\n",
        "That particular sentence doesn't make grammatical sense, and many of the generated sentences won't, but trust me, some of the outputs will astound and amaze you.\n",
        "\n",
        "Now, it's time for you to actually build a Markov chain yourself. Ba-bum-bum!"
      ],
      "metadata": {
        "id": "uggt9jVm-J1w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read input data\n",
        "\n",
        "What input data do you want to use for your Markov chain? It's up to you! Here are a few options:\n",
        "\n",
        "* [sayings.txt](https://gist.githubusercontent.com/pamelafox/ef698fe3a1a6950949e3987fe1303c07/raw/672570d0e002524f53c2f3b32dc0f70d905f5d1f/sayings.txt): ~2400 wise (or not so wise) sayings.\n",
        "* [titles.txt](https://gist.githubusercontent.com/pamelafox/cf569b133d717ab02c6635926ffc960a/raw/2c4913d8dd86a97ab602d48394638b24228e5ba2/movie_titles.txt): 6800 movie titles (which are on the shorter side, so your generated \"sentences\" will be quite short)\n",
        "* [composingprograms.txt](https://corise.com/static/course/introduction-to-python/assets/cky8zb2g800et1g4igmx1bjbz/python_textbook.txt): 3800 sentences from composingprograms.com, a textbook about Python, Scheme, and SQL.\n",
        "\n",
        "You're also welcome to use another data source, such as exporting your own social media posts or downloading a whole book from [Project Gutenberg](https://www.gutenberg.org/). Just be prepared to do a little more work cleaning the data into sentences.\n",
        "\n",
        "### ✏︎ For you to do\n",
        "\n",
        "The code below already opens a URL and stores its lines in a variable.\n",
        "\n",
        "* Change the URL to your preferred data source. If using the data sources above, right click and copy the link to get the full URL starting with `http`. If you've uploaded a new data source as a file to CoLab, change the code to instead use the `open()` function with the filename.\n",
        "* Check that `sentences[0]` and `len(sentences)` are what you expected them to be."
      ],
      "metadata": {
        "id": "sb5RIVMeBYT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import urllib.request\n",
        "\n",
        "# Open the file\n",
        "text_file = urllib.request.urlopen('http://replace.with.a/url')\n",
        "\n",
        "# Read the file into a string according to UTF-8 encoding\n",
        "text_contents = text_file.read().decode('utf-8')\n",
        "\n",
        "# Split the string into sentences (using newline as delimiter)\n",
        "sentences = text_contents.split('\\n')\n",
        "\n",
        "# Check a few sentences\n",
        "print(sentences[0])\n",
        "print(len(sentences))"
      ],
      "metadata": {
        "id": "pxeppWPqKwZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize the Markov chain\n",
        "\n",
        "For this project, you'll be storing the chain using a dictionary, with each key as a word, and each value as a list of words that come after that word. It's a great data structure for Markov chains since we can very quicky look up values by word.\n",
        "\n",
        "For example, the final chain for the simple toddler language would look like:\n",
        "\n",
        "```\n",
        "chain = {\n",
        "  \"START_OF_SENTENCE\": [\"I\"],\n",
        "  \"I\": [\"am\", \"am\", \"love\", \"want\", \"love\"],\n",
        "  \"am\": [\"mad\", \"happy\"],\n",
        "  \"love\": [\"you\", \"cookies\"],\n",
        "  \"want\": [\"cookies\"],\n",
        "  \"mad\": [\"END_OF_SENTENCE\"],\n",
        "  \"happy: [\"END_OF_SENTENCE\"],\n",
        "  \"you\": [\"END_OF_SENTENCE\"],\n",
        "  \"cookies\": [\"END_OF_SENTENCE\"]\n",
        "}\n",
        "```\n",
        "\n",
        "Every chain starts off with a word that isn't really a word - `START_OF_SENTENCE`. That key stores all the possible words that can start a sentence.\n",
        "\n",
        "As you can see, there's also a special not-word `END_OF_SENTENCE` indicating the end of the sentence, so that we know how often a word is followed by a period or new line.\n",
        "\n",
        "The values contain an entry for every time a word shows up after the key word, instead of storing the probability, simply because it's easier to code up that way. It'd also be possible to store the probabilities instead, if you wanted.\n",
        "\n",
        "\n",
        "### ✏︎ For you to do\n",
        "\n",
        "Run the code below to initialize the chain with the single starting key:"
      ],
      "metadata": {
        "id": "XYUPvcMgLPsb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = {\"START_OF_SENTENCE\": []}"
      ],
      "metadata": {
        "id": "Ebed06DQSFC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Wordify each sentence\n",
        "\n",
        "The `sentences` array right now contains strings that are entire sentences, but the chain dictionary needs keys and values that are words.\n",
        "\n",
        "In this step, you'll write a short function that can turn a sentence into a list of words. Use the string methods that we learned earlier in the week.\n",
        "\n",
        "### ✏︎ For you to do\n",
        "\n",
        "Implement `wordify_sentence` and run the doctests to make sure it works as expected.\n",
        "\n"
      ],
      "metadata": {
        "id": "bBi_jqrbait4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def wordify_sentence(sentence):\n",
        "  \"\"\" Splits a sentence into words (space-separated),\n",
        "  stripping off any periods at the end and lowercasing all the words.\n",
        "  Leading and trailing whitespace should also be stripped.\n",
        "\n",
        "  >>> wordify_sentence('Money burns a hole in your pocket.')\n",
        "  ['money', 'burns', 'a', 'hole', 'in', 'your', 'pocket']\n",
        "  >>> wordify_sentence(' Miss Jerry\\\\n ')\n",
        "  ['miss', 'jerry']\n",
        "  \"\"\"\n",
        "  # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "WKEYYWsEa-8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the tests when you're ready\n",
        "import doctest\n",
        "doctest.run_docstring_examples(wordify_sentence, globals(), verbose=True, name=\"wordify_sentence\")"
      ],
      "metadata": {
        "id": "cyYEvLL4i_Si"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the Markov chain (Part 1)\n",
        "\n",
        "In this step, you'll make a function that can take a single sentence and update the chain based on each word in the sentence. If the word is the first word in the sentence, then it belongs in the value of the \"START_OF_SENTENCE\" key. If the word is followed by another word, then that subsequent word needs to be added to its value. Otherwise, if it's the last word in the sentence, then \"END_OF_SENTENCE\" needs to be added to its value.\n",
        "\n",
        "One tricky thing to look out for: the chain only starts off with a single key, \"START_SENTENCE\", so none of the other words in the sentences will exist as keys in the dictionary yet. Before you can update the values for those other words, you'll need to add a key in the dictionary for them and initialize it with an empty list - but only if it didn't exist yet!\n",
        "\n",
        "### ✏︎ For you to do\n",
        "\n",
        "Implement the `add_sentence` function below, following the guidance in the comments. Run the doctests to see if it works as expected."
      ],
      "metadata": {
        "id": "_tPI8ajcXhBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def add_sentence(chain, sentence):\n",
        "  \"\"\"\n",
        "  >>> test_chain = {'START_OF_SENTENCE': []}\n",
        "  >>> add_sentence(test_chain, 'I am happy')\n",
        "  >>> test_chain\n",
        "  {'START_OF_SENTENCE': ['i'], 'i': ['am'], 'am': ['happy'], 'happy': ['END_OF_SENTENCE']}\n",
        "  >>> add_sentence(test_chain, 'I am')\n",
        "  >>> test_chain\n",
        "  {'START_OF_SENTENCE': ['i', 'i'], 'i': ['am', 'am'], 'am': ['happy', 'END_OF_SENTENCE'], 'happy': ['END_OF_SENTENCE']}\n",
        "  \"\"\"\n",
        "  # Split the sentence into a list of words\n",
        "  _______ = _______\n",
        "\n",
        "  # Loop through each word in the list\n",
        "  # (Using a while loop gives you a way to know\n",
        "  # if the word is the first or the last)\n",
        "  i = 0\n",
        "  while i < len(words):\n",
        "    word = words[i]\n",
        "\n",
        "    # Handle case of first word in sentence\n",
        "    if ________:\n",
        "      __________\n",
        "\n",
        "    # If word isn't in chain yet, add it as a key\n",
        "    if ________:\n",
        "      __________\n",
        "\n",
        "    # Now figure out what word to add to\n",
        "    # the list of values for this word\n",
        "  \n",
        "    # First, handle case of last word in sentence\n",
        "    if ____________:\n",
        "      _____________\n",
        "    # Otherwise, handle case of a word that has a word after\n",
        "    else:\n",
        "      ______________\n",
        "\n",
        "    i += 1\n"
      ],
      "metadata": {
        "id": "ihNP8dKcLPeS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the tests when you think add_sentence is working\n",
        "import doctest\n",
        "doctest.run_docstring_examples(add_sentence, globals(), verbose=True, name=\"add_sentence\")"
      ],
      "metadata": {
        "id": "BUlVuNmLiOWj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the Markov chain (Part 2)\n",
        "\n",
        "Now that you've got `add_sentence` working, it's time to call it on every sentence from your input data."
      ],
      "metadata": {
        "id": "EaM9i9rziyU0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✏︎ For you to do\n",
        "\n",
        "Use a loop to go through each sentence in `sentences` (the list of sentences stored above) and call `add_sentence` on each of them. Once you've done that, you can log out the entire `chain` dictionary to see what it looks like, or just a single key, like `chain[\"the\"]`."
      ],
      "metadata": {
        "id": "iGs1X8Wde4wD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR LOOP HERE"
      ],
      "metadata": {
        "id": "NGzn4p3Fe1yV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate new sentences\n",
        "\n",
        "Finally, the big pay-off! In this step, you'll implement the function `generate_sentence`. The first word in a sentence is always one of the words from the \"START_OF_SENTENCE\" key. Then, the next word must come from the words for that start word's key in the chain. Keep picking words until the chosen word is \"END_OF_SENTENCE\".\n",
        "\n",
        "At the beginning we said that Markov chains are probabilistic: some words are more likely to come after words than other words. How can you randomly choose words so that we're more likely to choose the most common words after? Well, since the values in the chain store a word each time it's seen, that means that randomly picking from that list of words _will_ end up picking those repeated words more often than others. So you can just use that handy `random.choice(list)` function on the list, and it'll just work.\n"
      ],
      "metadata": {
        "id": "sIsF6f89jXgm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def generate_sentence(chain):\n",
        "  \"\"\"\n",
        "  >>> test_chain = {'START_OF_SENTENCE': ['i'], 'i': ['am'], 'am': ['happy'], 'happy': ['END_OF_SENTENCE']}\n",
        "  >>> generate_sentence(test_chain)\n",
        "  'i am happy'\n",
        "  \"\"\"\n",
        "  # Initialize a list to hold the words for the new sentence\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "  # Select the first word randomly from all possible start words\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "  # Append the first word to the new sentence\n",
        "  # YOUR CODE HERE\n",
        "\n",
        "  # Keep looping, selecting the next word to come after\n",
        "  # and adding it to the new sentence\n",
        "  # until the word reached is \"END_OF_SENTENCE\"\n",
        "  # YOUR CODE HERE\n",
        "  while True:\n",
        "      # YOUR CODE HERE\n",
        "\n",
        "  # Return the sentence as a string (instead of a list of words)\n",
        "  # YOUR CODE HERE"
      ],
      "metadata": {
        "id": "HYqrZs2-NBLQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run the tests when you think add_sentence is working\n",
        "import doctest\n",
        "doctest.run_docstring_examples(generate_sentence, globals(), verbose=True, name=\"generate_sentence\")"
      ],
      "metadata": {
        "id": "rzkpa6LBrJWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ✏︎ For you to do\n",
        "\n",
        "Run the code below to generate a sentence with your chain. Each time you run it, you should see a new sentence. "
      ],
      "metadata": {
        "id": "ibAbP_APrKWh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_sentence(chain)"
      ],
      "metadata": {
        "id": "LLDoRq8QNz3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## You're done!\n",
        "\n",
        "🎉 Woohoo! 🥳 Project 3! 👏🏽 Complete! 🎊 \n",
        "\n",
        "Remember to share your project and your favorite generated sentences with your classmates. I can't wait to see them!"
      ],
      "metadata": {
        "id": "eMqMnefQvfWh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extensions\n",
        "\n",
        "If you enjoyed this project and want to take it further, there are many ways to affect the behavior of a Markov chain. Some ideas:\n",
        "\n",
        "* **New data source**: Try a different data source than the one you originally tested with, or go out and try to find a brand new data source. You might find text files online, you could fetch data from an API (like [The Movie Database API](https://developers.themoviedb.org/3)), or you could scrape data from a website using the [BeautifulSoup library](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) (check the website terms first).\n",
        "* **Better punctuation support**: What happens right now if a sentence contains commas, hyphens, or other sorts of punctuation? How does that affect the result of your chain? Think about what sort of string processing you might do to handle punctuation better, and whether you'd want to strip some punctuation entirely or store some punctuation symbols as \"words\" in the chain.\n",
        "* **Multiple sentence support**: Very related to above, what if a line contains multiple sentences? How will the chain currently treat that situation? See if you can come up with a more robust handling.\n",
        "* **Bigrams**: The chain is currently based on \"unigrams\": single words and their likelihood of being followed by another word. One way to make the chain produce more natural language is to instead use \"bigrams\" - pairs of words. The chain would store bigrams in the keys instead of single words. Then, when generating sentences, it would need to look up the next word based on the most recent _two_ words in the sentence, not just the most recent word. \n",
        "\n",
        "Enjoy, and let us know if you try out any of the extensions!"
      ],
      "metadata": {
        "id": "OZ3lfr3pQ2Nw"
      }
    }
  ]
}